<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Sam&#39;s Engineering Stuff</title>
    <link>/post/</link>
    <description>Recent content in Posts on Sam&#39;s Engineering Stuff</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 May 2025 13:07:02 -0400</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Physical Intelligence&#39;s FAST and the Discrete Cosine Transform</title>
      <link>/post/dct/</link>
      <pubDate>Wed, 21 May 2025 13:07:02 -0400</pubDate>
      <guid>/post/dct/</guid>
      <description>&lt;p&gt;I was recently reading about Physical Intelligence&amp;rsquo;s new-ish &lt;a href=&#34;https://www.physicalintelligence.company/research/fast&#34;&gt;FAST&lt;/a&gt; tokenizer. Formally, tokenizing an action looks like taking in a multidimensional fixed-length trajectory $\tau^{(i)} \in \mathbb{R}^D$, where $i$ is a timestep indexer and $D$ is the dimensionality of your space, and outputting a sequence of tokens whose length will almost always scale linearly with the length of the input. As an aside: you might choose to tokenize actions in joint space, which is more natural to control in, but it&amp;rsquo;s extremely common in generalist models like the ones Physical Intelligence is working on to work in a &amp;ldquo;task space&amp;rdquo; (read: Cartesian space) and handle robot morphology with traditional controls.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Robotaxi Economics</title>
      <link>/post/robotaxi-economics/</link>
      <pubDate>Fri, 09 May 2025 19:10:35 -0400</pubDate>
      <guid>/post/robotaxi-economics/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve learned a lot in the last few months about the economics of robotaxis, and I&amp;rsquo;d like to share a bit of it here as a way of accumulating it in one place. A robotaxi is a particular application of a self-driving car. Tesla wants to sell self-driving cars directly to consumers, but there are a few businesses (Waymo, Zoox, until recently Cruise) that are focusing exclusively on robotaxis, meaning that they are in the business of selling rides rather than cars. Although they focus on the same basic technology, these businesses turn out to be extremely different, and that&amp;rsquo;s what this post focuses on. I&amp;rsquo;m going to break it into pro-Waymo and pro-Tesla sections to be a little more provocative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequential Probability Ratio Test</title>
      <link>/post/sprt/</link>
      <pubDate>Fri, 07 Mar 2025 22:30:15 -0500</pubDate>
      <guid>/post/sprt/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a sort of interesting problem: suppose you want to test a drug to see if it&amp;rsquo;s effective. You decide to test the drug on 100 people, and when you get the results you see that it&amp;rsquo;s effective at $p=0.052$. Annoying! Under the conventional cutoff of $p &amp;lt; 0.05$ you can&amp;rsquo;t say that your result is significant. But you &lt;em&gt;also&lt;/em&gt; can&amp;rsquo;t test a few more people. The reason for this is that this would introduce a bias in your data: you&amp;rsquo;ve decided to add some more random noise to your data because you didn&amp;rsquo;t get the result you wanted, but you wouldn&amp;rsquo;t have done that if you had gotten $p=0.049$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bypassing Paste Restrictions</title>
      <link>/post/bypass-paste/</link>
      <pubDate>Fri, 21 Feb 2025 18:22:54 -0500</pubDate>
      <guid>/post/bypass-paste/</guid>
      <description>&lt;p&gt;I recently had to pay a tax bill online, but it turned out that my town&amp;rsquo;s bill payment website has one of those incredibly annoying JavaScript forms that blocks pasting into the bank account number field. This is so, so stupid. I genuinely don&amp;rsquo;t understand what benefit they think this could possibly offer. They force me to type it in manually, and then, because typing things in manually is an error-prone process, the next field forces me to type in the whole number manually again! You know what&amp;rsquo;s not an error-prone process? &lt;em&gt;Copying and pasting the account number.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Winkler Puzzle 1</title>
      <link>/post/winkler-puzzle-1/</link>
      <pubDate>Sat, 04 Jan 2025 01:24:41 -0500</pubDate>
      <guid>/post/winkler-puzzle-1/</guid>
      <description>&lt;p&gt;Happy new year! I&amp;rsquo;m reading Peter Winklers &amp;ldquo;Mathematical Puzzles: a Connoisseur&amp;rsquo;s Collection&amp;rdquo; and found this nice problem:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Prove that all natural numbers have a nonzero multiple whose base-10 representation consists only of zeros and ones&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Here&amp;rsquo;s the very elegant proof:&lt;/p&gt;&#xA;&lt;details&gt;&#xA;&lt;summary&gt;Show solution&lt;/summary&gt;&#xA;Given a natural number $n$, consider $\{111, 1111, 11111, \dots\}$, a set of &#39;all-ones&#39; numbers which begins with the first all-ones number greater than $n$ and contains $n+1$ elements, with each successive element adding a 1. Since there are only $n$ integers (modulo $n$) and the set has $n+1$ elements, it follows that at least two elements of the set must be congruent modulo $n$. If you subtract one from the other, you&#39;ll get a number congruent to zero modulo $n$ (a multiple of $n$) which begins with a string of ones and ends with a string of zeros.&#xA;&lt;/details&gt;&#xA;&lt;p&gt;I really enjoyed this one - it&amp;rsquo;s not terribly often that you see the pigeonhole principle come up in number theory. The problem is hard to solve any other way, but the trick in the solution makes it trivial. I&amp;rsquo;d highly recommend the book to anyone looking to do a little bit of math for fun!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kelly Criterion</title>
      <link>/post/kelly-criterion/</link>
      <pubDate>Sat, 26 Oct 2024 16:38:54 -0400</pubDate>
      <guid>/post/kelly-criterion/</guid>
      <description>&lt;p&gt;The Kelly criterion is a formula used in portfolio management and gambling which tells you how much of your portfolio to bet on a game with a certain payoff and probability of winning. Let&amp;rsquo;s derive it!&lt;/p&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll set up the problem as:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You have a bankroll of $W$&lt;/li&gt;&#xA;&lt;li&gt;Probability of winning $p$ (this is your privately estimated probability of victory)&lt;/li&gt;&#xA;&lt;li&gt;You&amp;rsquo;re offered odds of $(b-1):1$, so you will be paid back $b$ times your bet if you win (if you&amp;rsquo;re offered even odds, you will be paid back twice your bet by winning). We assume here that you lose your entire bet if you lose.&lt;/li&gt;&#xA;&lt;li&gt;You will bet $fW$ dollars, so $f$ is a fraction of your bankroll&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The expected value of your bankroll after the bet resolves is:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Steps Problem</title>
      <link>/post/steps_problem_finished/</link>
      <pubDate>Tue, 27 Aug 2024 17:04:34 -0400</pubDate>
      <guid>/post/steps_problem_finished/</guid>
      <description>&lt;p&gt;At the technical interview for my first college internship, I was asked to solve a few simple programming problems. The last one was this question:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;You can move forward in increments of one step or three steps. In how many different ways can you reach $n$ steps?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;I got completely &lt;a href=&#34;https://xkcd.com/356/&#34;&gt;nerd sniped&lt;/a&gt; by this question. Answering it is easy; there is a simple DP solution (below, using Python&amp;rsquo;s &lt;code&gt;functools.cache&lt;/code&gt; to do DP for me):&lt;/p&gt;</description>
    </item>
    <item>
      <title>MikTeX on Arch Linux</title>
      <link>/post/miktex/</link>
      <pubDate>Sun, 18 Aug 2024 22:33:50 -0400</pubDate>
      <guid>/post/miktex/</guid>
      <description>&lt;p&gt;I recently got a new laptop and had to reinstall latex. The default way to do this is to install TeXLive with pacman, which is packaged into a set of groups which contain anything from the most basic installation (&lt;code&gt;texlive-basic&lt;/code&gt;) to a virtually exhaustive installation (some combination of &lt;code&gt;texlive-latexextra&lt;/code&gt;, &lt;code&gt;texlive-fontsextra&lt;/code&gt; and a few others). I initially opted to install &lt;code&gt;texlive-latexrecommended&lt;/code&gt;, until I tried to compile a document and realized I was missing &lt;code&gt;fullpage.sty&lt;/code&gt;. This was only available in &lt;code&gt;texlive-latexextra&lt;/code&gt;, and I also needed a font that was only present in &lt;code&gt;texlive-fontsextra&lt;/code&gt;. So to get about 200 kB of .sty files, I was supposed to install &amp;gt;2 GB of packages.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Data Driven Control</title>
      <link>/post/data-driven-control/</link>
      <pubDate>Sat, 27 Jan 2024 21:31:28 -0500</pubDate>
      <guid>/post/data-driven-control/</guid>
      <description>&lt;p&gt;This essay continues my review of Steve Brunton’s textbook, &lt;a href=&#34;https://faculty.washington.edu/sbrunton/DataBookV2.pdf&#34;&gt;“Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.”&lt;/a&gt;. This covers chapter 10, on data-driven control. Sadly, it is not all about MPC, but we&amp;rsquo;ll cover all of it anyway.&lt;/p&gt;&#xA;&lt;h1 id=&#34;mpc&#34;&gt;MPC&lt;/h1&gt;&#xA;&lt;p&gt;The essence of MPC is to optimize a trajectory of control values by solving a nonlinear program:&lt;/p&gt;&#xA;&lt;p&gt;$$\begin{align}&#xA;\min_{u_{t:T}} \quad &amp;amp;J(x_{t:T}, u_{t:T})\quad \texttt{s.t.} \\&#xA;x_{t+1} &amp;amp;= f(x_{t}, u_{t}) \\&#xA;&amp;amp;\dots&#xA;\end{align}$$&#xA;Where $J$ is some cost function on the trajectory of states $x_{t:T}$ and the trajectory of control values $u_{t:T}$. There might be arbitrary constraints: on the set of valid states, on the control bounds, on how much the control values change between timesteps, etc. You can then start rolling out your control trajectory as you re-solve the problem on a moving horizon basis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Balanced Realizations</title>
      <link>/post/balanced-realizations/</link>
      <pubDate>Sun, 14 Jan 2024 13:55:01 -0500</pubDate>
      <guid>/post/balanced-realizations/</guid>
      <description>&lt;p&gt;Welcome to the tenth installment of my review of &lt;a href=&#34;https://faculty.washington.edu/sbrunton/DataBookV2.pdf&#34;&gt;&amp;ldquo;Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control&amp;rdquo;&lt;/a&gt; (technically a review of chapter 9, because one chapter got split into two reviews). This review gives a high-level overview of the math behind balancing tranformations.&lt;/p&gt;&#xA;&lt;p&gt;Suppose you have a typical linear system which you&amp;rsquo;re observing somehow and potentially also exerting control on, as we&amp;rsquo;ve been discussing for several chapters:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{align}&#xA;\dot{x} &amp;amp;= Ax + Bu \\&#xA;y &amp;amp;= Cx + Du&#xA;\end{align}&#xA;$$&#xA;It might be useful to you to find some reduced-order input-output model based on a projection of $x$, given as $x = T \tilde{x}$. This gives rise to a new set of matrices such that:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Linear Control Theory 2</title>
      <link>/post/linear-control-theory-2/</link>
      <pubDate>Wed, 20 Dec 2023 11:47:00 -0500</pubDate>
      <guid>/post/linear-control-theory-2/</guid>
      <description>&lt;p&gt;&lt;em&gt;Welcome back to the Brunton Textbook Review series, where I&amp;rsquo;m reviewing all of Steve Brunton&amp;rsquo;s textbook, “Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.” This post will conclude chapter 8, on linear control systems. The next post will cover Chapter 9, on system identification and balanced representation.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;optimal-control&#34;&gt;Optimal Control&lt;/h1&gt;&#xA;&lt;p&gt;In &lt;a href=&#34;https://sscheele.github.io/post/linear-control-theory/&#34;&gt;part 1&lt;/a&gt; of this review of the Linear Control Theory chapter, we found that you can stabilize a linear system by defining a control law $u = Kx$, such that $A - BK$ has negative eigenvalues (or, more precisely, eigenvalues with negative real components). But there are an infinite number of negative eigenvalues, and we can choose any of them! Eigenvalues that are more negative will cause the system decay faster, but may actually lead it to be less robust (huge control efforts are more likely to make your system encounter unmodeled and possibly nonlinear dynamics, like back EMF, friction, slippage, flexion in your components, etc). Optimal control tells us which $K$ matrix we should choose.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multimodal LLMs and DALL-E Explained</title>
      <link>/post/multimodal-llms/</link>
      <pubDate>Sun, 26 Nov 2023 16:45:54 -0500</pubDate>
      <guid>/post/multimodal-llms/</guid>
      <description>&lt;p&gt;&lt;em&gt;I&amp;rsquo;m finding that writing here is a great way to make sure I have a complete understanding of various concepts. With that in mind, I decided to keep going along the LLM line. Scheduled programming on the Brunton textbook will probably resume soon.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;I imagined there would be some cool trick to multimodal LLMs, but that doesn&amp;rsquo;t actually seem to be the case! If there is a cool trick, it&amp;rsquo;s the cool trick of realizing that you can do an unusual amount of things with transformers, including image processing, which itself lets you do a surprising number of things.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Llama Writeup</title>
      <link>/post/llama/</link>
      <pubDate>Sun, 19 Nov 2023 17:31:30 -0500</pubDate>
      <guid>/post/llama/</guid>
      <description>&lt;p&gt;This post is an attempt to explain Facebook&amp;rsquo;s Llama LLM to myself.&lt;/p&gt;&#xA;&lt;h1 id=&#34;part-1-transformer-models-architecture&#34;&gt;Part 1: Transformer Models, Architecture&lt;/h1&gt;&#xA;&lt;h2 id=&#34;tokenization-and-initial-embedding&#34;&gt;Tokenization and Initial Embedding&lt;/h2&gt;&#xA;&lt;p&gt;First, the input string is split into tokens. These tokens are often &amp;ldquo;subword tokens,&amp;rdquo; meaning that word parts (like -ed and -tion) get their own tokens. This is useful in the case that a word is out-of-vocabulary (OOV). If the input is nonsense, the tokens may be individual characters. The tokens are then looked up in a dictionary of embeddings. This dictionary is &lt;em&gt;not&lt;/em&gt; static, however! It is typically randomly initialized at the start training, then the vector associated with each token is learned as part of the training process&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backup Script</title>
      <link>/post/backup-script/</link>
      <pubDate>Mon, 09 Oct 2023 10:41:17 -0400</pubDate>
      <guid>/post/backup-script/</guid>
      <description>&lt;p&gt;I recently had my laptop&amp;rsquo;s SSD fail, and it took me a long time to get back up and running - partially because I tried to install and run Wayland instead of X (which worked out reasonably well - I ended up switching back to X mostly because OnlyOffice was super slow on Wayland/sway, but it seems to me like Wayland is coming within a couple of years!). But a lot of the issue was that I hadn&amp;rsquo;t made a proper backup in years, so I needed to remember how I had set up a ton of different software. I&amp;rsquo;d been using CloneZilla, which is basically a live disk that lets you clone entire disks/partitions, for backups, but there were some problems:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Linear Control Theory</title>
      <link>/post/linear-control-theory/</link>
      <pubDate>Sun, 08 Oct 2023 20:43:47 -0400</pubDate>
      <guid>/post/linear-control-theory/</guid>
      <description>&lt;p&gt;&lt;em&gt;This essay continues my review of Steve Brunton&amp;rsquo;s textbook, &lt;a href=&#34;https://faculty.washington.edu/sbrunton/DataBookV2.pdf&#34;&gt;&amp;ldquo;Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.&amp;rdquo;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;When talking about control, we&amp;rsquo;ll mostly discuss closed-loop feedback control, but it&amp;rsquo;s worth briefly emphasizing that there are actually other kinds of control which are sometimes appropriate. For example, most traffic lights are purely on timers, with no sensing capabilities at all. This sort of nonreactive, pre-programmed control is called open loop. However, you might change the light timings and flow of traffic in a city close to a major sporting event. This sort of control, where you make changes to what is still essentially an open-loop control law based on exogenous changes to the system, is called disturbance feedforward control.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Data Driven Dynamics</title>
      <link>/post/data-driven-dynamics/</link>
      <pubDate>Thu, 21 Sep 2023 08:48:12 -0400</pubDate>
      <guid>/post/data-driven-dynamics/</guid>
      <description>&lt;p&gt;Welcome! This post is my first review in the Dynamics and Control section of &lt;a href=&#34;https://faculty.washington.edu/sbrunton/DataBookV2.pdf&#34;&gt;&amp;ldquo;Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This chapter focuses on analyzing dynamical systems by pure observation (that is, deducing their properties purely by observing the system, rather then from some first-principles analysis). First, it introduces some basic principles of dynamical systems, then gives a few methods for discovering their dynamics.&lt;/p&gt;&#xA;&lt;p&gt;There are a few things we want to do with dynamical systems:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Compressed Sensing</title>
      <link>/post/compressed-sensing/</link>
      <pubDate>Tue, 19 Sep 2023 08:31:33 -0400</pubDate>
      <guid>/post/compressed-sensing/</guid>
      <description>&lt;p&gt;This essay continues my review of Steve Brunton&amp;rsquo;s textbook, &lt;a href=&#34;https://faculty.washington.edu/sbrunton/DataBookV2.pdf&#34;&gt;&amp;ldquo;Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;A while ago, I wanted to do machine learning on some time-series data, but I had a problem: some timesteps in the (otherwise regularly-sampled) dataset had been removed for being &amp;ldquo;low-quality.&amp;rdquo; I couldn&amp;rsquo;t get the original data, but I also couldn&amp;rsquo;t figure out a good way to interpolate the missing data. At the time, I think I just backfilled the data with the next non-null value, but in retrospect, it probably would have been a good idea to turn to compressed sensing. In this review, we&amp;rsquo;ll first briefly review sparsity, which is an essential concept for compressed sensing, and then turn to compressed sensing itself. Then we&amp;rsquo;ll learn some variations and applications of compressed sensing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - Fourier and Wavelet Transforms</title>
      <link>/post/fourier-wavelet/</link>
      <pubDate>Wed, 05 Jul 2023 19:00:39 -0400</pubDate>
      <guid>/post/fourier-wavelet/</guid>
      <description>&lt;p&gt;Welcome! This post, on the Fourier and wavelet transforms, is a continuation of my series summarizing the new edition of Steve Brunton&amp;rsquo;s textbook, &amp;ldquo;Data-Driven Science and Engineering.&amp;rdquo; In the current edition, this post covers pages 64-117.&lt;/p&gt;&#xA;&lt;p&gt;You can think of the Fourier transform as a specific kind of projection, similar to a vector projection:&lt;/p&gt;&#xA;&lt;p&gt;$$\text{proj}_{v}(u) = \frac{v}{v \cdot v} u \cdot v$$&lt;/p&gt;&#xA;&lt;p&gt;That&amp;rsquo;s how you project a vector $u$ onto a vector $v$, but it relies on the dot product. If we wanted to project a function $f$ onto another function $g$, we&amp;rsquo;ll first need the equivalent of a dot product for functions. If we sampled $f$ and $g$ over some $n$ points on some domain $[a, b]$, we could construct vectors and dot them:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brunton Textbook Review - SVD Interpretations</title>
      <link>/post/svd-interp/</link>
      <pubDate>Tue, 27 Jun 2023 12:17:39 -0400</pubDate>
      <guid>/post/svd-interp/</guid>
      <description>&lt;p&gt;Steve Brunton recently released the second edition of his textbook, entitled &amp;ldquo;Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control.&amp;rdquo; Since ML, dynamical systems, and control are three of my favorite things to learn about, I&amp;rsquo;ve decided to read all 761 pages of the book and do a writeup in many parts. This is the first part, and will cover interpretations of the Singular Value Decomposition (SVD), pages 1-55 of the book.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning ML 1</title>
      <link>/post/learning-ml-1/</link>
      <pubDate>Tue, 06 Jun 2023 11:49:09 -0400</pubDate>
      <guid>/post/learning-ml-1/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve decided to get a lot better at ML, and I enjoyed my Rust learning log so much that I&amp;rsquo;m going to do something similar for this. However, I think there will be a lot of differences between learning ML and learning Rust. Rust is an easier learning environment in some important ways:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Causal models&lt;/strong&gt; - In Rust, we have a reasonably good idea of what&amp;rsquo;s going on, and we can look things up when we don&amp;rsquo;t. We can understand, for example, why we can&amp;rsquo;t take a second mutable reference to some object: this would violate the invariants of the borrow checker. If we want to understand why the borrow checker is set up this way, we can do that. In ML, most explanations for why something works well are post-hoc, and solutions built from first principles fail more often than they succeed. &lt;em&gt;We can&amp;rsquo;t simply learn the underlying logic of ML to predict what will and won&amp;rsquo;t work.&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Iteration speed&lt;/strong&gt; - Rust allows you to fail even faster than most compiled languages, because so many errors are caught by the compiler, and it also gives you incredibly helpful error messages. In ML, on the other hand, slow training makes it hard to test your ideas, and when they fail you typically don&amp;rsquo;t know why.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I expect that learning ML will be a lot like learning to be a mason or herbalist used to be: lots of arcane tricks without much grounding in a causal system, with expensive experimentation getting in the way of rapid improvement. Learning masonry or herbalism(?) on your own is not the move: you need to apprentice yourself to someone and watch how they do it. Absent that, here are some of my ideas:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kalman Filtering: Part 3</title>
      <link>/post/kf-3/</link>
      <pubDate>Thu, 25 May 2023 23:39:41 -0400</pubDate>
      <guid>/post/kf-3/</guid>
      <description>&lt;p&gt;Welcome to my third Kalman Filtering post! We &lt;a href=&#34;../kf-2&#34;&gt;left off&lt;/a&gt; on a cliffhanger. How will the LUMVE algorithm perform against Double EWMA?&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./img/lumve_full.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Pretty well! In the graph, DEWMA starts out with a better estimate than LUMVE, but LUMVE stays pretty consistently closer to ground truth by being initially more willing to update based on new measurements and later more reluctant to fit noise. Of course, DEWMA has tunable parameters that let it be more willing to update, too. In fact, this graph is in some ways a good illustration of why you might prefer DEWMA: it has just two simple formulas, but it performs almost as well as the Kalman filter on this example! DEWMA has two parameters you need to define, $\alpha$ and $\beta$ - in setting up a Kalman filter even on a 2D state space, you might easily define 10 magic numbers without seeing a huge improvement. But you do get a payoff for all of those constants in the form of greater accuracy and flexibility. Additionally, this system is probably &lt;em&gt;way&lt;/em&gt; more friendly to DEWMA than you&amp;rsquo;ll typically see in real life: while KF may leave you wondering just how many first-order Taylor approximations we should really be letting Rudolph Kalman get away with here, those approximations are usually good enough in practice (when they&amp;rsquo;re not, nonlinear extensions of KF like EKF and UKF are often effective). What&amp;rsquo;s often not good enough is DEWMA&amp;rsquo;s approach of effectively ignoring the measurement and system dynamics functions. If we were dealing with, for example, a pendulum, DEWMA would have a really hard time, while KF would handle it like a champ (probably - though with oscillators you also have to take into account questions like your sampling rate vs the oscillation frequency).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kalman Filtering: Part 2</title>
      <link>/post/kf-2/</link>
      <pubDate>Sun, 21 May 2023 18:14:41 -0400</pubDate>
      <guid>/post/kf-2/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;../kf-1&#34;&gt;my last Kalman filtering post&lt;/a&gt;, we derived a LUMVE (Linear, Unbiased, Minimum-Variance) estimator for a static object. Then, we showed that we could use that general estimator to make an estimator that incorporates &lt;em&gt;a priori&lt;/em&gt; information (a prior best guess) by sort of pretending our prior guess was a measurement. You might be able to guess where we go next: if we can find the covariance matrix $\hat{P}$ for &lt;em&gt;that&lt;/em&gt; estimator, then we could take our &lt;em&gt;last&lt;/em&gt; estimation and use it as the &lt;em&gt;a priori&lt;/em&gt; guess for our &lt;em&gt;next&lt;/em&gt; estimation. In other words, we could filter iteratively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Double and Triple Exponential Smoothing: Part 1</title>
      <link>/post/triple-exponential-smoothing/</link>
      <pubDate>Tue, 16 May 2023 15:38:24 -0400</pubDate>
      <guid>/post/triple-exponential-smoothing/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s say you have some time-series data and you want to smooth it out and maybe run some prediction. You could design a FIR filter or something, but FIR filters involve a number of design decisions. What will your cutoff points be? What window size will you use? How will you handle data before your first full window? But, perhaps most importantly: if your data starts to move, how will you handle it? For example, let&amp;rsquo;s say we&amp;rsquo;re designing a robot to follow a wall at a certain distance, and controlling the steering angle $\theta$. In order to do this, you&amp;rsquo;re using a rangefinder to measure your current distance from the wall, but of course the data you get from it will be noisy, or you wouldn&amp;rsquo;t be smoothing it in the first place. If you start to drift towards the wall at a constant angle, your range data will start to exhibit a linear trend. If you&amp;rsquo;re expecting this, you have a couple of options if you decide to go the FIR filter route. If you&amp;rsquo;re being fancy, you could run a changepoint detection algorithm that triggers a linear fit being run on the data after the detected change, then subtract out the trendline, filter the residual, and add the trendline back in. But your pipeline has now become complicated, heavyweight, and arguably relies on too many magic numbers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Rust: Week 4</title>
      <link>/post/learning-rust-4/</link>
      <pubDate>Sun, 14 May 2023 12:46:22 -0400</pubDate>
      <guid>/post/learning-rust-4/</guid>
      <description>&lt;p&gt;This will be the last installment in my series on learning Rust, in which I give some notes on &lt;code&gt;Cow&lt;/code&gt;. I finished re-implementing &lt;code&gt;Cow&lt;/code&gt; for myself and found some performance stuff that surprised me! As my main testbed, I&amp;rsquo;m using this function:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn test_speed() {&#xA;    use rand::{thread_rng, Rng};&#xA;    use std::time::{Duration, Instant};&#xA;    use std::vec::*;&#xA;&#xA;    let mut rng = thread_rng();&#xA;    let pos_nums: Vec&amp;lt;i32&amp;gt; = (1..1000000).collect();&#xA;    let neg_nums: Vec&amp;lt;i32&amp;gt; = (-50..999950).collect();&#xA;&#xA;    let t_start = Instant::now();&#xA;    // naive method&#xA;    for _ in 1..10 {&#xA;        let mut mystery_arr = match rng.gen_bool(0.9) {&#xA;            true =&amp;gt; pos_nums.clone(),&#xA;            false =&amp;gt; neg_nums.clone(),&#xA;        };&#xA;        for i in 0..mystery_arr.len() {&#xA;            if mystery_arr[i] &amp;lt; 0 {&#xA;                mystery_arr[i] = -mystery_arr[i];&#xA;            }&#xA;        }&#xA;    }&#xA;    println!(&amp;quot;Time elapsed (naive): {:?}&amp;quot;, t_start.elapsed());&#xA;&#xA;    let t_start = Instant::now();&#xA;    // cow method&#xA;    for _ in 1..10 {&#xA;        let mut mystery_arr = match rng.gen_bool(0.9) {&#xA;            true =&amp;gt; MyCow::Borrowed(&amp;amp;pos_nums),&#xA;            false =&amp;gt; {&#xA;                println!(&amp;quot;MyCow must clone&amp;quot;);&#xA;                MyCow::Borrowed(&amp;amp;neg_nums)&#xA;            }&#xA;        };&#xA;        for i in 0..mystery_arr.len() {&#xA;            if mystery_arr[i] &amp;lt; 0 {&#xA;                mystery_arr.to_mut()[i] = -mystery_arr[i];&#xA;            }&#xA;        }&#xA;    }&#xA;    println!(&amp;quot;Time elapsed (my cow): {:?}&amp;quot;, t_start.elapsed());&#xA;&#xA;    use std::borrow::Cow;&#xA;    let t_start = Instant::now();&#xA;    // cow method&#xA;    for _ in 1..10 {&#xA;        let mut mystery_arr = match rng.gen_bool(0.9) {&#xA;            true =&amp;gt; Cow::Borrowed(&amp;amp;pos_nums),&#xA;            false =&amp;gt; {&#xA;                println!(&amp;quot;Cow must clone&amp;quot;);&#xA;                Cow::Borrowed(&amp;amp;neg_nums)&#xA;            }&#xA;        };&#xA;        for i in 0..mystery_arr.len() {&#xA;            if mystery_arr[i] &amp;lt; 0 {&#xA;                mystery_arr.to_mut()[i] = -mystery_arr[i];&#xA;            }&#xA;        }&#xA;    }&#xA;    println!(&amp;quot;Time elapsed (cow): {:?}&amp;quot;, t_start.elapsed());&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is taken pretty much exactly from the docs, and it will tell us how long each method (naive, Cow, and MyCow) took and how many borrows had to happen. All results are reported using debug compile targets, though I found that the release targets were consistently around 10x faster. And the result is&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Animated Plot Collection</title>
      <link>/post/sys_anim/</link>
      <pubDate>Sat, 13 May 2023 19:42:24 -0400</pubDate>
      <guid>/post/sys_anim/</guid>
      <description>&lt;p&gt;I took a break from learning Rust to make &lt;a href=&#34;https://github.com/sscheele/sys_anim&#34;&gt;a repository of animated plots&lt;/a&gt; using Matplotlib.&lt;/p&gt;&#xA;&lt;p&gt;I was initially going to use Rust for this, but ended up deciding not to - though &lt;a href=&#34;https://github.com/plotters-rs/plotters/&#34;&gt;plotters&lt;/a&gt; looks very cool! This is mostly because I came to the conclusion that no Rust library is mature enough to do what I want, which is to make nice-looking stuff for one-off use cases like presentations. A Rust solution would probably be faster and more maintainable, but these Python scripts are simple and easy to extend, which is a better value proposition for one-off projects.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Rust: Week 3</title>
      <link>/post/learning-rust-3/</link>
      <pubDate>Thu, 11 May 2023 19:05:41 -0400</pubDate>
      <guid>/post/learning-rust-3/</guid>
      <description>&lt;p&gt;This week I&amp;rsquo;m focusing on implementing my own version of &lt;code&gt;Cow&lt;/code&gt;, which I&amp;rsquo;m increasingly convinced is helpful even though I haven&amp;rsquo;t made much progress on it.&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-ive-done&#34;&gt;What I&amp;rsquo;ve Done&lt;/h1&gt;&#xA;&lt;h2 id=&#34;rusts-smart-pointers-and-cow&#34;&gt;Rust&amp;rsquo;s Smart Pointers and &lt;code&gt;Cow&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Rust has raw pointers that you can pass around the way you&amp;rsquo;d pass around C pointers, but you&amp;rsquo;re not supposed to use them except in very specific circumstances. Raw pointers in Rust are unsafe and sort of ruin the point of following all of the ownership rules. Instead, you&amp;rsquo;re generally supposed to use smart pointers, the simplest of which is &lt;code&gt;Box&lt;/code&gt;, which just creates a heap-allocated variable to which all the usual Rust ownership rules apply. It&amp;rsquo;s a smart pointer in that the heap memory will be deallocated when the &lt;code&gt;Box&lt;/code&gt; falls out of scope.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Rust: Week 2</title>
      <link>/post/learning-rust-2/</link>
      <pubDate>Sun, 30 Apr 2023 19:45:48 -0400</pubDate>
      <guid>/post/learning-rust-2/</guid>
      <description>&lt;p&gt;This week of Rust was focused almost entirely on last week&amp;rsquo;s technique #4, open source contribution! I spent the week working on three issues.&lt;/p&gt;&#xA;&lt;h2 id=&#34;issue-1-documentation&#34;&gt;Issue #1: &lt;a href=&#34;https://github.com/helix-editor/helix/issues/6691&#34;&gt;Documentation&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;This issue dealt with documenting two specific commands which had been added to Helix as part of a recent update that introduced soft line wrapping. I proposed extending the documentation generator to automatically document all commands, but then it turned out that someone was already doing the same thing (but probably better than I would have), so I just manually updated the docs. A nice first issue, I didn&amp;rsquo;t have to write any code but ended up reading some in order to locate the docstrings, which can only be found in the source right now.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Rust: Week 1</title>
      <link>/post/learning-rust-1/</link>
      <pubDate>Sun, 23 Apr 2023 23:56:06 -0400</pubDate>
      <guid>/post/learning-rust-1/</guid>
      <description>&lt;p&gt;This is the first in (hopefully) a series of posts about my efforts to learn Rust! The goal is to track my progress and maybe see what techniques work better/worse.&lt;/p&gt;&#xA;&lt;h1 id=&#34;thoughts-on-rust&#34;&gt;Thoughts on Rust&lt;/h1&gt;&#xA;&lt;p&gt;Rust is a very cool language and definitely worth learning, if you already know C. If you don&amp;rsquo;t already know C, I think it would probably be a lot more frustrating. This is because a lot of Rust&amp;rsquo;s design, such as the ownership system, is based on high-level principles which are designed to eliminate problems you&amp;rsquo;d encounter in C. If you don&amp;rsquo;t know about those problems, the ownership system will feel dumb and obstructive instead of a cool formalization of good memory management practice. There are a lot of other things like this too - Rust concepts will feel natural, elegant, and expressive if you already understand the problems they were designed to address, but will feel unintuitive and pointless if you&amp;rsquo;re new to systems programming, because it&amp;rsquo;s much easier to map from systems constraints to Rust design principles than it is to map the other way.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Point Bw Points</title>
      <link>/post/point-bw-points/</link>
      <pubDate>Tue, 28 Mar 2023 16:32:27 -0400</pubDate>
      <guid>/post/point-bw-points/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s another short post on a problem I solved today. Suppose you have a point and want to see if it&amp;rsquo;s between two other points. If you&amp;rsquo;re working in a 2D space, you can use a solution along the lines of &lt;a href=&#34;https://bryceboe.com/2006/10/23/line-segment-intersection-algorithm/&#34;&gt;this one&lt;/a&gt; from Bryce Boe, but it&amp;rsquo;s not obvious how it would generalize to higher dimensions.&lt;/p&gt;&#xA;&lt;p&gt;So I thought I&amp;rsquo;d take a different approach: what do we mean when we say a point is &amp;ldquo;between two points?&amp;rdquo; It can help to visualize the set of points between, say, (0,0) and (1,1):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacobian Trajectory Generation</title>
      <link>/post/jacobian-traj/</link>
      <pubDate>Thu, 23 Mar 2023 14:08:19 -0400</pubDate>
      <guid>/post/jacobian-traj/</guid>
      <description>&lt;p&gt;I wanted to make a mini-post with a sort of cute problem I solved today: let&amp;rsquo;s say you want your robotic arm to travel from one joint-space position to another, but along the way you want it to follow a Cartesian end-effector trajectory. This actually isn&amp;rsquo;t such an unrealistic scenario! You&amp;rsquo;re basically constraining the start and end to be &amp;ldquo;sort of reasonable&amp;rdquo; (something you might want to do, given that 7+dof arms can have weird/slow ik), but you only care about the end effector path in between them, not the whole joint trajectory. My solution to this is to iteratively:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Problem 14 Part 1</title>
      <link>/post/problem-14-1/</link>
      <pubDate>Sun, 19 Mar 2023 16:49:35 -0400</pubDate>
      <guid>/post/problem-14-1/</guid>
      <description>&lt;p&gt;I recently saw a post on &lt;a href=&#34;https://gwern.net/problem-14&#34;&gt;Gwern&amp;rsquo;s blog&lt;/a&gt; on an interesting problem in probability and expected value:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;A shuffled deck of cards has an equal number of red and black cards. When you draw a red card from the deck, you get \$1, and when you draw a black card you lose \$1. You can stop whenever you want. If the deck has $N$ cards, what is the expected value of playing this game? When should you stop?&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Latex Packages</title>
      <link>/post/latex-packages/</link>
      <pubDate>Tue, 14 Mar 2023 22:03:48 -0400</pubDate>
      <guid>/post/latex-packages/</guid>
      <description>&lt;p&gt;I like to have the ability to run things locally on my computer, but getting LaTeX to work has always been a pain because of the package system. It tells me that I&amp;rsquo;m missing packages, but they aren&amp;rsquo;t on the AUR and I&amp;rsquo;ve never been willing to invest the time to figure out how to install them. But now that I&amp;rsquo;m working on my thesis I devoted the time to figure it out and I&amp;rsquo;m posting it here so I don&amp;rsquo;t forget.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kalman Filtering 1: Motivation and LUMVE</title>
      <link>/post/kf-1/</link>
      <pubDate>Sun, 12 Mar 2023 13:44:51 -0400</pubDate>
      <guid>/post/kf-1/</guid>
      <description>&lt;p&gt;Welcome back to my Kalman filtering notes! In this post we&amp;rsquo;ll first back up and talk about when and why we would want to use a Kalman filter, and we&amp;rsquo;ll go all the way to deriving a filter with the same properties as KF.&lt;/p&gt;&#xA;&lt;h1 id=&#34;kalman-filter-setup-and-motivation&#34;&gt;Kalman Filter Setup and Motivation&lt;/h1&gt;&#xA;&lt;p&gt;Suppose you&amp;rsquo;re designing a satellite that needs to estimate its location in space very precisely. The two most obvious ways to do this are to use GPS and to infer your location by measuring it relative to background stars. Suppose for a moment that your satellite is totally immobile and can take a number of independent measures of its altitude, which we&amp;rsquo;ll call $y_i, i \in 1..m$. Each of these measurements can also have some uncertainty associated with it, which we&amp;rsquo;ll describe using measurement variances $\sigma^2_i$. If we want the estimate with the minimum variance, it&amp;rsquo;s hopefully intuitive that we can find it by taking an average weighted by inverse variance:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kalman Filtering: Intro</title>
      <link>/post/kf-intro/</link>
      <pubDate>Fri, 10 Mar 2023 22:38:16 -0500</pubDate>
      <guid>/post/kf-intro/</guid>
      <description>&lt;p&gt;This is the first in a series of posts on the Kalman Filter! Over the series, we&amp;rsquo;ll first derive a filter with the same properties as the KF, then the KF itself, and finally more advanced variants such as EKF and UKF.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll assume for this series that you have some knowledge of linear algebra and probability theory. However, a quick refresher never hurt anyone, so the remainder of this post will cover that. It will take the form of sparse notes, since it&amp;rsquo;s intended to be an information-dense set of reminders/reference sheet rather than a beginner&amp;rsquo;s introduction to linear algebra.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Puzzle: Counting to 100</title>
      <link>/post/100-puzzle/</link>
      <pubDate>Tue, 07 Mar 2023 13:18:41 -0500</pubDate>
      <guid>/post/100-puzzle/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a puzzle I found in &amp;ldquo;The Moscow Puzzles&amp;rdquo;, by Boris Kordemsky and Martin Gardner:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Person 1 can call any number from 1 to 10. Person 2 can then increment the last called number by any whole number from 1 to 10, then it&amp;rsquo;s Person 1&amp;rsquo;s turn again, and so on. The first person to call 100 wins. Give a strategy that allows Person 1 to win this game every time&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hello, world!</title>
      <link>/post/test/</link>
      <pubDate>Tue, 07 Mar 2023 12:30:35 -0500</pubDate>
      <guid>/post/test/</guid>
      <description>&lt;p&gt;Hello, world!&lt;/p&gt;&#xA;&lt;p&gt;I made this site based on a very minimal hugo theme. Despite the fact that this theme is only 150 lines of code, I still don&amp;rsquo;t understand it! There are two reasons for this:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I&amp;rsquo;m not familiar with Hugo (the static site generator I&amp;rsquo;m using) or really web development&lt;/li&gt;&#xA;&lt;li&gt;There seem to be a lot of magic constants in hugo that I don&amp;rsquo;t like. I think this was part of an effort to make Hugo &amp;ldquo;creator-friendly&amp;rdquo; by hiding complexity from users of the tool. But hidden complexity is still there! For example, the homepage of this site renders by including some custom headers and footers (which are easily visible in the source), a list of posts (again, visible in the source), and custom content from &amp;ldquo;content/_index.md&amp;rdquo;, a filename that is completely invisible to the user. Apparently this is just hardcoded into Hugo. If you make the mistake of editing &amp;ldquo;content/index.md&amp;rdquo; instead, the page won&amp;rsquo;t render correctly and there&amp;rsquo;s nothing visible in the source or configuration files that you can use to trace the problem through the build process. Eventually, I may choose to migrate this site to another tool to avoid this problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;In the meantime, my priority was to get MathJax working so I can do math on here:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
